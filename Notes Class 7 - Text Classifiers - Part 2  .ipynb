{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"dcDm6T3HaRYG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716365926145,"user_tz":-120,"elapsed":19296,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"6263161a-d381-43db-e605-c3d7dd0f7945"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Import drive\n","from google.colab import drive\n","#Mount Google Drive\n","ROOT=\"/content/drive\"\n","drive.mount(ROOT, force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6Zxub8uiRF7M","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1716365930403,"user_tz":-120,"elapsed":385,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"52fe99fc-fa71-48b3-8523-e6273c0ab9cb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["%pwd"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ajyqZyUQRLdL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716365934620,"user_tz":-120,"elapsed":201,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"facdea7b-8a6c-47a6-8e2f-2544ec8882f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/dlss24\n"]}],"source":["%cd /content/drive/MyDrive/dlss24"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"LTf-v1gWRNv2","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1716365935924,"user_tz":-120,"elapsed":5,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"0f886853-6226-4643-b943-6f6a1a203481"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/dlss24'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["%pwd"]},{"cell_type":"markdown","metadata":{"id":"4lnmzacMRV9e"},"source":["# Data\n","\n","Download the corpora data from: https://codeocean.com/capsule/0078777/tree/v1\n","\n","Uploaded to a location outside your gitHub (to do not be tracked)\n","\n","You pwd should have the files:\n","\n","\n","1.   source_corpus.csv\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BE3M-5mHbRdU"},"source":["\n","# Module IV:  - Class 7 - Text Classifiers\n","\n"]},{"cell_type":"markdown","metadata":{"id":"T5LxOrR6bjFY"},"source":["# Reading data"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"TmnDoKifpFZ7","executionInfo":{"status":"ok","timestamp":1716365939927,"user_tz":-120,"elapsed":839,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}}},"outputs":[],"source":["import pandas as pd\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"eOaGLl-Lbxjf","executionInfo":{"status":"ok","timestamp":1716365943447,"user_tz":-120,"elapsed":2443,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}}},"outputs":[],"source":["df_source_corpus=pd.read_csv('/content/drive/MyDrive/dlss24/source_corpus.csv')"]},{"cell_type":"code","source":["df_source_corpus = df_source_corpus.dropna(subset=['text'])"],"metadata":{"id":"V1zJsjYHxx77","executionInfo":{"status":"ok","timestamp":1716365945604,"user_tz":-120,"elapsed":206,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"N1JiJSClKR2M","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1716365948256,"user_tz":-120,"elapsed":232,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"9ea868a2-25ae-4252-869d-bcc3de30acde"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                   topic_44                topic_8  \\\n","0                 democracy  freedom and democracy   \n","1  freedom and human rights  freedom and democracy   \n","2             law and order      fabric of society   \n","3                  no topic               no topic   \n","4          civic mindedness      fabric of society   \n","\n","                                                text  \n","0  Two hundred summers ago, this Democratic Party...  \n","1  In 1992, the party Thomas Jefferson founded in...  \n","2  Our land reverberates with a battle cry of fru...  \n","3                     America is on the wrong track.  \n","4                   The American people are hurting.  "],"text/html":["\n","  <div id=\"df-11c2841d-b3da-46a5-8280-ae77d4d0eee9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>topic_44</th>\n","      <th>topic_8</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>democracy</td>\n","      <td>freedom and democracy</td>\n","      <td>Two hundred summers ago, this Democratic Party...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>freedom and human rights</td>\n","      <td>freedom and democracy</td>\n","      <td>In 1992, the party Thomas Jefferson founded in...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>law and order</td>\n","      <td>fabric of society</td>\n","      <td>Our land reverberates with a battle cry of fru...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>no topic</td>\n","      <td>no topic</td>\n","      <td>America is on the wrong track.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>civic mindedness</td>\n","      <td>fabric of society</td>\n","      <td>The American people are hurting.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11c2841d-b3da-46a5-8280-ae77d4d0eee9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-11c2841d-b3da-46a5-8280-ae77d4d0eee9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-11c2841d-b3da-46a5-8280-ae77d4d0eee9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-12c5dc8e-4d36-4292-b565-3c64cd67b9c6\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-12c5dc8e-4d36-4292-b565-3c64cd67b9c6')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-12c5dc8e-4d36-4292-b565-3c64cd67b9c6 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_source_corpus"}},"metadata":{},"execution_count":8}],"source":["df_source_corpus.head()"]},{"cell_type":"code","source":["df_source_corpus.shape"],"metadata":{"id":"PQKMyozRLKTz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716366136919,"user_tz":-120,"elapsed":6,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"0d374936-e9f2-42d8-a859-eec771f9978a"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(115408, 3)"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["# Word2vec\n","\n","[Image of word2vec 1-hidden layer NN](https://becominghuman.ai/mathematical-introduction-to-glove-word-embedding-60f24154e54c)\n","\n","Word2vec creates vectors that represent the context of words, while GloVe creates vectors that represent the co-occurrence of words.\n","\n","Word2vec uses a shallow neural network to create vectors, while GloVe uses a global matrix factorization technique\n","\n","\n","Word2vec requires a large amount of training data, while GloVe can be trained on smaller datasets. This makes GloVe more suitable for smaller tasks, while Word2vec is better suited for larger applications."],"metadata":{"id":"jgYz_Kq2sd8x"}},{"cell_type":"code","source":["# word2vec requires sentences as input\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from string import punctuation\n","translator = str.maketrans('','',punctuation)\n","from nltk.corpus import stopwords\n","stoplist = set(stopwords.words('english'))\n","from nltk.stem import SnowballStemmer\n","stemmer = SnowballStemmer('english')"],"metadata":{"id":"x_LiEdoZshp7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716366358950,"user_tz":-120,"elapsed":374,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"1013428c-f291-4115-bf0a-9c3cf471886f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["#from previous class\n","def normalize_text(doc):\n","    \"Input doc and return clean list of tokens\"\n","    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n","    lower = doc.lower() # all lower case\n","    nopunc = lower.translate(translator) # remove punctuation\n","    words = nopunc.split() # split into tokens\n","    nostop = [w for w in words if w not in stoplist] # remove stopwords\n","    no_numbers = [w if not w.isdigit() else '#' for w in nostop] # normalize numbers\n","    stemmed = [stemmer.stem(w) for w in no_numbers] # stem each word\n","    return stemmed"],"metadata":{"id":"McKq2yIqslqN","executionInfo":{"status":"ok","timestamp":1716366360059,"user_tz":-120,"elapsed":2,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["\n","1. Apply the normalize_text function to the data (df_source_corpus)\n","2. Train word2vec on all of the data (All that is required is that the input yields one sentence (list of utf8 words) after another.)\n","\n"],"metadata":{"id":"tOQZ7jdpyECO"}},{"cell_type":"code","source":["df_source_corpus['tokens_cleaned'] = df_source_corpus['text'].apply(normalize_text)"],"metadata":{"id":"_9vEJPOaRHbt","executionInfo":{"status":"ok","timestamp":1716366628900,"user_tz":-120,"elapsed":26094,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["from gensim.models import Word2Vec"],"metadata":{"id":"QxSXPBV-RHZB","executionInfo":{"status":"ok","timestamp":1716368089745,"user_tz":-120,"elapsed":2,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# Prepare sentences for Word2Vec (list of lists of words)\n","sentences = df_source_corpus['tokens_cleaned'].tolist()\n","\n","# Train the Word2Vec model\n","model = Word2Vec(sentences, # list of tokenized sentences\n","                 vector_size=300, # word vector dimensionality\n","                 window=5, # consider 5 words after and before (10 words of context)\n","                 min_count=25, # minimum word count\n","                 workers=4,\n","                 sample = 1e-3) # downsample setting for frequent words\n","\n","# Save the model\n","model.save(\"word2vec.model\")"],"metadata":{"id":"Um4uhXZ9RHWY","executionInfo":{"status":"ok","timestamp":1716366911121,"user_tz":-120,"elapsed":17638,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","> 1. Look for the word vectors for immigration, economy, security and rights\n","2. See which of these (economy, security and rights) is closer to immigration\n","\n","Try w2v.wv.\n","\n"],"metadata":{"id":"pAi5kOetjPYT"}},{"cell_type":"code","source":["vocab = set(model.wv.index_to_key)\n"],"metadata":{"id":"suANhb-SRKk-","executionInfo":{"status":"ok","timestamp":1716367776214,"user_tz":-120,"elapsed":4,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["word = \"immi\"\n","if word in vocab:\n","    print(f\"'{word}' is in the vocabulary.\")\n","else:\n","    print(f\"'{word}' is not in the vocabulary.\")"],"metadata":{"id":"3BjjMCSvRKaX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716368135880,"user_tz":-120,"elapsed":5,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"a8450fc0-712e-4223-ff50-5d594c5f2575"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["'immi' is not in the vocabulary.\n"]}]},{"cell_type":"code","source":["[word for word in vocab if word.startswith(\"imm\")] #its immigr"],"metadata":{"id":"_qd2rxcjRKTB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716368215088,"user_tz":-120,"elapsed":205,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"ab17cc5a-e384-4292-ad72-13b53039bf34"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['immunistaion',\n"," 'immin',\n"," 'immor',\n"," 'immeasur',\n"," 'immeasurablybi',\n"," 'immens',\n"," 'immedi',\n"," 'immediatelypolicingcrimin',\n"," 'immigrationrel',\n"," 'immigrant”',\n"," 'immunolog',\n"," 'immers',\n"," 'immigrants”',\n"," 'immunis',\n"," 'immort',\n"," 'immun',\n"," 'imma',\n"," 'immigr']"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["[word for word in vocab if word.startswith(\"secur\")] #its security"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8N97vvnhPln","executionInfo":{"status":"ok","timestamp":1716368328107,"user_tz":-120,"elapsed":218,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"7eb17a2b-07e5-44b6-ebd7-75368ad65f8f"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['securityclear',\n"," 'security—serv',\n"," 'secur',\n"," 'securityfrom',\n"," 'security”',\n"," 'security—',\n"," 'securityto']"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["[word for word in vocab if word.startswith(\"econ\")] #its immigr"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uW1A1eJf9Tr","executionInfo":{"status":"ok","timestamp":1716368274784,"user_tz":-120,"elapsed":5,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"baf19ec6-0b5b-46c6-e1bb-b6f80e45ca97"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['economy–',\n"," 'econveyanc',\n"," 'economy”',\n"," 'economist',\n"," 'economy–which',\n"," 'economi',\n"," 'economy—and',\n"," 'economicgrowth',\n"," 'econ',\n"," 'economy1600',\n"," 'economywid',\n"," 'economics”',\n"," 'economy…',\n"," 'economy–th',\n"," 'economydestroy',\n"," 'econom',\n"," 'economy—includ',\n"," 'economy–that',\n"," 'economy–a',\n"," 'economiesto']"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":[],"metadata":{"id":"pOlYUFo_RKNv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","> Analogies with the dataset:\n","1. Scientist is to man as __ is to woman\n","2. Scientist is to woman as __ is to man\n","\n"],"metadata":{"id":"pFmF2Otwj_ku"}},{"cell_type":"code","source":[],"metadata":{"id":"ag9NHqcMRLOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IlV1sqxHRLLA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bgJwJXwSRLIU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GLove\n","\n"],"metadata":{"id":"GcaseSp126K2"}},{"cell_type":"markdown","source":["\n","\n","> Train Glove on your dataset and check most similar words to environment\n","\n"],"metadata":{"id":"qLaY8FBqpTqW"}},{"cell_type":"code","source":["!pip install glove-python3"],"metadata":{"id":"RqCdsLVb3UV9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import itertools\n","from glove import Corpus, Glove"],"metadata":{"id":"CAB9XU2c3Bpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus = Corpus()\n","corpus.fit(sentences_normalized, window=10)\n","glove = Glove(no_components=100, learning_rate=0.05)"],"metadata":{"id":"f3Jox1Tc273w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n","glove.add_dictionary(corpus.dictionary)"],"metadata":{"id":"6tB7T1gO3z4r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glove.word_vectors[glove.dictionary['environment']].shape"],"metadata":{"id":"CXBikr0kpgKB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glove.word_vectors[glove.dictionary['environment']]"],"metadata":{"id":"aqkLH5Xy3uZy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glove.most_similar('environment')"],"metadata":{"id":"EeriekgC3yP8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","# 2017: Attention is all you need\n","\n"],"metadata":{"id":"eHP6ewXACXpC"}},{"cell_type":"code","source":["# Attention Basically\n","import numpy as np\n","\n","class Node:\n","\n","  def __init__(self):\n","\n","    # the vector stored at this node (what is learned, the output of the layer)\n","    self.data=np.random.randn(20)\n","\n","    # weights governing how this node interacts with other nodes\n","    self.wkey = np.random.randn(20, 20)\n","    self.wquery = np.random.randn(20, 20)\n","    self.wvalue = np.random.randn(20, 20)\n","\n","  def key(self):\n","    # what do I have?\n","    return self.wkey @ self.data\n","\n","  def query(self):\n","    # what am I looking for? (next work is the text shift by 1, classification is the class)\n","    return self.wquery @ self.data\n","\n","  def value(self):\n","    # what do I oublicly reveal to others?\n","    return self.wvalue @ self.data\n"],"metadata":{"id":"l6oXzqVbCXRT","executionInfo":{"status":"ok","timestamp":1716370639728,"user_tz":-120,"elapsed":244,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["class Graph:\n","\n","  def __init__(self):\n","    # make 10 nodes\n","    self.nodes=[Node() for _ in range(10)]\n","    #make 40 edges\n","    randi=lambda: np.random.randint(len(self.nodes))\n","    self.edges=[[randi(),randi()] for _ in range(40)]\n","\n","  def run(self):\n","\n","    updates=[]\n","    for i,n in enumerate(self.nodes):\n","\n","      #what is the node looking for?\n","      q=n.query()\n","\n","      #find all edges that are input to this node\n","      inputs = [self.nodes[ifrom] for (ifrom, ito) in self.edges if ito==i]\n","      if len(inputs)==0:\n","        continue #ignore, next in for loop\n","      # gather keys, i.e what they hold\n","      keys=[m.key() for m in inputs]\n","      #calculate compatibilities: dot product of  key with query\n","      scores=[k.dot(q) for k in keys]\n","      #softmax them so they sum to 1\n","      scores=np.exp(scores)\n","      scores=scores/np.sum(scores)\n","      # gather the appropriate values with weighted sum\n","      values=[m.value() for m in inputs]\n","      update=sum([s*v for s,v in zip(scores, values)])\n","      updates.append(update)\n","\n","    for n,u in zip(self.nodes, updates):\n","      n.data=n.data +u #residual connection"],"metadata":{"id":"LK4Nz5KYEa_L","executionInfo":{"status":"ok","timestamp":1716370641962,"user_tz":-120,"elapsed":303,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xk4SNQfuIbn0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","> 1. Create a Graph\n","2. Print the key, value and query for the 4th node\n","3. Print the data before running and after running\n","\n"],"metadata":{"id":"Q118hX6lIrPl"}},{"cell_type":"code","source":["# Create a graph\n","graph = Graph()"],"metadata":{"id":"c4yTzX1IQhQQ","executionInfo":{"status":"ok","timestamp":1716370729433,"user_tz":-120,"elapsed":199,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# Print the key, value, and query for the 4th node\n","fourth_node = graph.nodes[3]  # Indexing starts from 0\n","print(\"Key for 4th node:\", fourth_node.key())\n","print(\"Value for 4th node:\", fourth_node.value())\n","print(\"Query for 4th node:\", fourth_node.query())"],"metadata":{"id":"k-jzsaiBQhNQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716370732084,"user_tz":-120,"elapsed":201,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"a4c88b99-fb3e-4778-9fa2-34860d31be8a"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Key for 4th node: [ -4.20254775   5.87475277   7.24250178   4.30217532   2.16178805\n","  -4.19815433  -9.17740384   2.79282711   1.05454066   1.45650278\n","  -5.3225827   -1.80318893   3.56746215  -3.24329846 -14.09618774\n","  -3.75720761   1.50816784  -2.40488704  -2.57727479   4.04193791]\n","Value for 4th node: [ 0.70898696 -5.21231372  3.37917067  2.44384225  6.01544466 -6.01617458\n","  4.04281367 -0.20406051  3.9103778  -1.72574992 -5.78883459 -8.30961984\n","  6.75526804  4.07006735  2.4729935   6.99721359  6.63685493 -5.81242095\n","  6.5760745   7.33311961]\n","Query for 4th node: [ 2.41123071  1.73617749 -0.04827642  3.06985727 -9.97973293  2.69368658\n"," -2.02686033  2.95545303 -1.82046462 -2.19683393  6.28456455  8.97763778\n"," -2.93982462 -2.36202515 -1.28880076 -6.17308813 -1.05500995 -2.35344872\n"," -3.38261425 -0.93667802]\n"]}]},{"cell_type":"code","source":["# Print the data before running\n","print(\"\\nData before running:\")\n","for i, node in enumerate(graph.nodes):\n","    print(f\"Node {i+1} data:\", node.data)\n","\n","# Run the graph\n","graph.run()\n","\n","# Print the data after running - updated based on dot product of attention and key\n","print(\"\\nData after running:\")\n","for i, node in enumerate(graph.nodes):\n","    print(f\"Node {i+1} data:\", node.data)"],"metadata":{"id":"neHQKNaiQhKO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716370735676,"user_tz":-120,"elapsed":238,"user":{"displayName":"Susana Tavares","userId":"13212507747979531014"}},"outputId":"840034b5-c912-4596-8262-defdb17edf51"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Data before running:\n","Node 1 data: [ 1.60443268 -0.19416816 -0.76957722 -1.2778503  -0.46745973 -0.1113019\n","  0.45755088  0.68440127 -1.03165548  0.74770867  0.20104499  0.08186741\n","  2.26998752 -2.73742026  0.32603983 -1.81663925  1.70729561 -1.48440702\n","  1.59137799 -0.15901859]\n","Node 2 data: [ 0.37913185  0.25373282  0.38933361 -0.61441881 -0.43056088 -1.18611862\n","  1.70676405  0.63473702 -0.24999738  0.19446626 -0.6384945   0.43668033\n","  0.41054996  1.47941884 -0.71743178  2.16205851 -1.35714345 -1.47621814\n"," -0.96075815  1.2596931 ]\n","Node 3 data: [ 0.23584829  0.12143856 -0.19413447  0.23370507 -2.40763706  1.12352786\n","  1.07711129  1.46724627  0.41695941  0.60198756 -0.79420364  0.9142507\n"," -0.14217512 -1.11138154  0.18890144 -0.13557909  0.47352984 -0.67899978\n"," -0.9871552  -0.35139409]\n","Node 4 data: [-0.34573238  1.14855812 -1.40752114 -1.85996526  0.74464511  1.35506532\n","  1.53109682 -1.12104992  1.50517596 -0.74537674  0.61179194  0.82686513\n"," -1.51093838 -0.90937394  0.85311312  0.53179353  1.3851931  -0.97749591\n","  0.0687939  -0.95868574]\n","Node 5 data: [-1.1460052  -0.32528094 -0.9512749  -1.00238692  0.76478392  1.4760568\n"," -0.20187365 -0.2362126   0.51265067  0.03938024  1.46316625  2.09988742\n"," -0.77861661  2.07326194 -0.19662597 -1.36291508  1.40294438  0.52389237\n","  0.91604986  0.82453073]\n","Node 6 data: [ 0.49106887  0.90622337 -0.64332852 -0.74592558 -1.08302366 -1.69332839\n"," -0.45496241 -0.12426492  0.5250676   0.73341031 -0.120118    1.25735648\n"," -0.12749163 -0.06354749  0.27436286  1.43930418  0.73749433 -1.00358376\n","  0.30644079  0.05628042]\n","Node 7 data: [-0.319874    1.37478859 -0.00746444 -0.53532482 -0.76126835 -1.00586784\n","  1.29635367  0.81907334 -0.58930872 -0.6670066  -1.01678232 -1.52863732\n"," -0.76704285  0.13590943 -0.98164163  2.60932904 -0.84586775 -1.37341368\n","  0.21643034  0.86542264]\n","Node 8 data: [-2.39403613  0.46130518 -2.23252104  0.1090308  -1.25711972 -0.65772253\n","  1.4216586  -0.76422838  1.19936199  0.58699937  0.32919657  0.0770844\n"," -1.37475826  0.48037642 -1.01644233  0.5853821   1.21330593 -0.21364257\n"," -0.0028355   0.16784679]\n","Node 9 data: [-0.07115962  0.12232861  0.01460952  2.28417638 -0.09227033 -1.71487375\n","  1.04127848 -0.67952445  0.05136315  0.25818491 -0.83610752 -0.32727182\n"," -1.2690354   0.21374659 -0.43786265 -0.18125184  1.55974589 -0.2220573\n","  0.95770386  0.38200463]\n","Node 10 data: [ 1.45898168 -1.21618151  1.19674432 -0.66022495  2.01436489  0.41415966\n"," -0.2324356   0.92420033  1.19940408  0.98084345  0.37970079  0.60198933\n"," -0.60324982 -0.22024654  2.25658258  1.00394659 -0.45804959 -0.96897562\n","  1.11598792 -0.78454001]\n","\n","Data after running:\n","Node 1 data: [ 0.49644436 -8.01762895 -8.22470716 10.76495084  7.84407258  1.17094028\n"," -1.099086   -0.66418276  4.9011502  -0.36381726  2.47759753 -1.90068398\n","  0.80417989  0.96768163 -6.30931305  2.61145739 -0.21475557  1.77482957\n"," -2.16135617  7.48173747]\n","Node 2 data: [ 0.9739723  -6.69977283  1.03549882  5.32387301 -0.18895577  0.49295228\n"," -2.24835713  1.76249883  3.41432681  7.66285599  6.49723276  2.67457779\n","  0.70964755  4.75956391  4.60714724  0.7485907   1.10500085 -2.32439634\n","  0.95586299  0.63616429]\n","Node 3 data: [-0.87220536 -7.70205624 -7.64941348 12.27656628  5.90391405  2.40571818\n"," -0.47964135  0.11860977  6.34988525 -0.50963183  1.48238179 -1.06830465\n"," -1.60803226  2.59384437 -6.44657069  4.2925697  -1.44852645  2.58026976\n"," -4.73993388  7.28940077]\n","Node 4 data: [ 0.07104388  0.48433093  0.43678186 -9.70133022  2.97213373  1.72307748\n","  9.85614169  1.46457518 -0.54289444  0.34082039  4.63680531  6.48948673\n"," -4.17750525 -0.08744014 -1.91264109 -2.09560973  3.51090783 -1.01825465\n"," -8.80064856  4.09044806]\n","Node 5 data: [-3.35779275  7.26106294 -7.63708804 12.55330732  3.84427948  4.84209143\n"," -4.19892194  0.47961876  1.29509995 -8.49802936  4.1638056  -3.57728034\n"," -2.05221946  6.90541684 -1.41223493 -1.64361536  6.00837318  5.5527937\n","  6.58356856 -5.40987556]\n","Node 6 data: [-3.1694083   4.27259071  1.80611999  2.28149683 -3.40468583  2.05076284\n","  4.92843534  2.56029237 -1.66941935 -5.00957484  6.88393502  3.71838685\n","  5.22215689  5.83866703  3.81117547  4.04775756  0.15720454 -1.31333088\n","  8.01378758  1.37555205]\n","Node 7 data: [-1.42792765 -6.44870622 -7.46274345 11.50753638  7.55028276  0.27632248\n"," -0.26039897 -0.52956315  5.34361711 -1.77862599  1.25980311 -3.51119267\n"," -2.23289999  3.84113534 -7.61711375  7.03747784 -2.76792404  1.88585586\n"," -3.53634834  8.50621751]\n","Node 8 data: [-3.80342372  7.6183063  -7.35268408 -5.87156015  1.50208427 -1.89584034\n","  4.91400654 -2.56462793 -0.49390517 -5.03371748 -3.32234831 -0.87458934\n"," -3.56746902  5.46826579 -0.8334208  -0.12071822  0.23768142  1.89849726\n","  2.71048031 -8.30108598]\n","Node 9 data: [-1.48054721  7.27932972 -5.10555353 -3.69641456  2.66693365 -2.95299156\n","  4.53362643 -2.479924   -1.64190402 -5.36253194 -4.4876524  -1.27894556\n"," -3.46174615  5.20163596 -0.25484112 -0.88735215  0.58412138  1.89008253\n","  3.67101967 -8.08692814]\n","Node 10 data: [ 6.06426087 -6.06534643  6.77905961  6.12851915  8.68100844  6.23188625\n","  8.33511017  4.16356313 -3.37674381  8.04297252 -0.21993101 -1.03443058\n","  2.26171103 -7.36106659  6.04976497  0.87083769 -1.93334592 -0.59221174\n","  1.2571448   3.46283457]\n"]}]},{"cell_type":"markdown","source":["# Transformers from scratch\n","\n","[Documentation](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)"],"metadata":{"id":"eNN6CACNJHPr"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math"],"metadata":{"id":"7aMFni29OkMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the scaled dot-product function\n","def scaled_dot_product(q, k, v, mask=None):\n","    d_k = q.size()[-1]\n","    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n","    attn_logits = attn_logits / math.sqrt(d_k)\n","    if mask is not None:\n","        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n","    attention = F.softmax(attn_logits, dim=-1)\n","    values = torch.matmul(attention, v)\n","    return values, attention\n","\n","# Define the expand mask function\n","def expand_mask(mask):\n","    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n","    if mask.ndim == 3:\n","        mask = mask.unsqueeze(1)\n","    while mask.ndim < 4:\n","        mask = mask.unsqueeze(0)\n","    return mask"],"metadata":{"id":"zUxmTfWCOpH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the MultiheadAttention class\n","class MultiheadAttention(nn.Module):\n","    def __init__(self, input_dim, embed_dim, num_heads):\n","        super().__init__() # we don't start from scratch, it gives all classes?\n","        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.qkv_proj = nn.Linear(input_dim, 3 * embed_dim)\n","        self.o_proj = nn.Linear(embed_dim, embed_dim)\n","        self._reset_parameters()\n","\n","    def _reset_parameters(self):\n","        nn.init.xavier_uniform_(self.qkv_proj.weight)\n","        self.qkv_proj.bias.data.fill_(0)\n","        nn.init.xavier_uniform_(self.o_proj.weight)\n","        self.o_proj.bias.data.fill_(0)\n","\n","    def forward(self, x, mask=None, return_attention=False):\n","        batch_size, seq_length, _ = x.size()\n","        if mask is not None:\n","            mask = expand_mask(mask)\n","        qkv = self.qkv_proj(x)\n","        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n","        qkv = qkv.permute(0, 2, 1, 3)\n","        q, k, v = qkv.chunk(3, dim=-1)\n","        values, attention = scaled_dot_product(q, k, v, mask=mask) # because it is a transformer it has this, mask is to adapt to how many multiheads I have\n","        values = values.permute(0, 2, 1, 3).reshape(batch_size, seq_length, self.embed_dim)\n","        o = self.o_proj(values)\n","        if return_attention:\n","            return o, attention\n","        else:\n","            return o\n","\n","# Define the EncoderBlock class\n","class EncoderBlock(nn.Module):\n","    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n","        super().__init__()\n","        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n","        self.linear_net = nn.Sequential(\n","            nn.Linear(input_dim, dim_feedforward),\n","            nn.Dropout(dropout),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(dim_feedforward, input_dim)\n","        )\n","        self.norm1 = nn.LayerNorm(input_dim)\n","        self.norm2 = nn.LayerNorm(input_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        attn_out = self.self_attn(x, mask=mask)\n","        x = x + self.dropout(attn_out)\n","        x = self.norm1(x)\n","        linear_out = self.linear_net(x)\n","        x = x + self.dropout(linear_out)\n","        x = self.norm2(x)\n","        return x\n","\n","# Define the TransformerEncoder class\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, num_layers, **block_args):\n","        super().__init__()\n","        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n","\n","    def forward(self, x, mask=None):\n","        for l in self.layers:\n","            x = l(x, mask=mask)\n","        return x\n","\n","    def get_attention_maps(self, x, mask=None):\n","        attention_maps = []\n","        for l in self.layers:\n","            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n","            attention_maps.append(attn_map)\n","            x = l(x)\n","        return attention_maps\n","\n","# Define the PositionalEncoding class\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe, persistent=False)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return x\n","\n","\n"],"metadata":{"id":"yXSNrmYyJJVq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the Transformer-based text classifier\n","class TransformerClassifier(nn.Module):\n","    def __init__(self, vocab_size, d_model, num_heads, num_layers, dim_feedforward, num_classes, max_len=5000, dropout=0.1):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n","        self.positional_encoding = PositionalEncoding(d_model, max_len)\n","        self.encoder = TransformerEncoder(\n","            num_layers=num_layers,\n","            input_dim=d_model,\n","            num_heads=num_heads,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout\n","        )\n","        self.pre_classifier = nn.Linear(d_model, d_model)\n","        self.classifier = nn.Linear(d_model, num_classes)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input_ids, mask=None):\n","        x = self.embedding(input_ids)\n","        x = self.positional_encoding(x)\n","        x = self.encoder(x, mask=mask)\n","        x = x[:, 0]  # Use the [CLS] token (first token) for classification\n","        x = F.relu(self.pre_classifier(x))\n","        x = self.dropout(x)\n","        logits = self.classifier(x)\n","        return logits\n"],"metadata":{"id":"pa2X_-W6O1BL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Example usage\n","vocab_size = 30522  # Vocabulary size (BERT's vocab size)\n","d_model = 768  # Embedding size\n","num_heads = 12  # Number of attention heads\n","num_layers = 6  # Number of transformer layers\n","dim_feedforward = 3072  # Feedforward network hidden size\n","num_classes = 2  # Number of output classes (e.g., binary classification)\n","max_len = 512  # Maximum sequence length\n","dropout = 0.1  # Dropout rate\n","\n","model = TransformerClassifier(vocab_size, d_model, num_heads, num_layers, dim_feedforward, num_classes, max_len, dropout)\n","\n","# Assume we have a tokenizer that converts sentences to input_ids\n","# For demonstration, using random input_ids\n","input_ids = torch.randint(0, vocab_size, (1, max_len))  # Batch size 1, sequence length max_len\n","mask = (input_ids != 0).unsqueeze(1).unsqueeze(2)  # Mask for non-padding tokens\n","\n","logits = model(input_ids, mask)\n","print(logits)"],"metadata":{"id":"5EGiWisKO4DQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mask.shape"],"metadata":{"id":"0sMeqFc6PjkF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#batch size, seq length\n","input_ids.shape\n","\n","\n","#In the attention mechanism,\n","#the mask needs to be broadcastable to the shape [batch_size, num_heads, seq_length, seq_length]\n","#[batch_size, 1, seq_length, seq_length]: the num_heads will be matched in the code by the number of heads"],"metadata":{"id":"RcKvWciHPtmh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformers: Hugging Face\n","\n","[Hugging Face](https://huggingface.co/docs/transformers/main_classes/tokenizer)"],"metadata":{"id":"LmhLfblw52Xf"}},{"cell_type":"code","source":["#!pip install transformers\n","import torch\n","from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n","\n","# gpu or cpu?\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print (device)"],"metadata":{"id":"WWNSmGAR4qlQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'distilbert-base-uncased' # huggingface model_ID or path to folder\n","model = DistilBertForSequenceClassification.from_pretrained(model_name)\n","print (model)"],"metadata":{"id":"fWibqqkV55fv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","> Apply that to our dataset\n","\n"],"metadata":{"id":"KkjQopMgQn4q"}},{"cell_type":"code","source":[],"metadata":{"id":"QQOP3CxaQngY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","> Manifesto Berta\n","\n","[Documentation](https://manifesto-project.wzb.eu/information/documents/manifestoberta)\n","\n"],"metadata":{"id":"knnoJW4D9eVv"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"manifesto-project/manifestoberta-xlm-roberta-56policy-topics-sentence-2023-1-1\")\n","tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n","\n","sentence = \"We will restore funding to the Global Environment Facility and the Intergovernmental Panel on Climate Change, to support critical climate science research around the world\"\n","\n","inputs = tokenizer(sentence,\n","                   return_tensors=\"pt\",\n","                   max_length=200,  #we limited the input to 200 tokens during finetuning\n","                   padding=\"max_length\",\n","                   truncation=True\n","                   )\n","\n","logits = model(**inputs).logits\n","\n","probabilities = torch.softmax(logits, dim=1).tolist()[0]\n","probabilities = {model.config.id2label[index]: round(probability * 100, 2) for index, probability in enumerate(probabilities)}\n","probabilities = dict(sorted(probabilities.items(), key=lambda item: item[1], reverse=True))\n","print(probabilities)\n","# {'501 - Environmental Protection: Positive': 67.28, '411 - Technology and Infrastructure': 15.19, '107 - Internationalism: Positive': 13.63, '416 - Anti-Growth Economy: Positive': 2.02...\n","\n","predicted_class = model.config.id2label[logits.argmax().item()]\n","print(predicted_class)\n","# 501 - Environmental Protection: Positive\n"],"metadata":{"id":"YgH0DoHb9g0S"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}